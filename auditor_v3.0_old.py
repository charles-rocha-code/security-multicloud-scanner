import os
import json
import datetime
import xml.etree.ElementTree as ET
import re
import hashlib
from collections import Counter, defaultdict
from typing import Optional, Dict, List, Tuple
from urllib.parse import urlparse

import requests

# =========================================
# CONFIGURAÃ‡Ã•ES E PATHS
# =========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMPLATE_FILE = os.path.join(BASE_DIR, "templates", "dashboard.html")
REPORT_FOLDER = os.path.join(BASE_DIR, "reports")
HISTORY_FOLDER = os.path.join(REPORT_FOLDER, "history")

os.makedirs(REPORT_FOLDER, exist_ok=True)
os.makedirs(HISTORY_FOLDER, exist_ok=True)


# =========================================
# PADRÃ•ES SENSÃVEIS EXPANDIDOS
# =========================================
SENSITIVE_PATTERNS = {
    # Chaves AWS
    "aws_access_key": re.compile(r'(AKIA[0-9A-Z]{16})', re.IGNORECASE),
    "aws_secret_key": re.compile(r'aws_secret_access_key["\']?\s*[:=]\s*["\']?([a-zA-Z0-9/+=]{40})', re.IGNORECASE),
    
    # Chaves privadas
    "private_key": re.compile(r'-----BEGIN.*PRIVATE KEY-----', re.IGNORECASE),
    "rsa_key": re.compile(r'-----BEGIN RSA PRIVATE KEY-----', re.IGNORECASE),
    "openssh_key": re.compile(r'-----BEGIN OPENSSH PRIVATE KEY-----', re.IGNORECASE),
    
    # API Keys
    "api_key": re.compile(r'api[_-]?key["\']?\s*[:=]\s*["\']?[a-zA-Z0-9_\-]{20,}', re.IGNORECASE),
    "bearer_token": re.compile(r'bearer\s+[a-zA-Z0-9_\-\.]{20,}', re.IGNORECASE),
    
    # Senhas
    "password": re.compile(r'password["\']?\s*[:=]\s*["\']?[^\s]{8,}', re.IGNORECASE),
    "db_password": re.compile(r'(DB|DATABASE)_PASSWORD["\']?\s*[:=]\s*["\']?[^\s]{8,}', re.IGNORECASE),
    
    # Tokens
    "token": re.compile(r'token["\']?\s*[:=]\s*["\']?[a-zA-Z0-9]{20,}', re.IGNORECASE),
    "jwt": re.compile(r'eyJ[a-zA-Z0-9_-]*\.eyJ[a-zA-Z0-9_-]*\.[a-zA-Z0-9_-]*', re.IGNORECASE),
    
    # ConexÃµes de banco de dados
    "connection_string": re.compile(r'(mongodb|mysql|postgresql|postgres):\/\/[^\s]+', re.IGNORECASE),
    
    # Chaves de serviÃ§os populares
    "github_token": re.compile(r'gh[pousr]_[a-zA-Z0-9]{36,}', re.IGNORECASE),
    "slack_token": re.compile(r'xox[baprs]-[a-zA-Z0-9-]+', re.IGNORECASE),
    "stripe_key": re.compile(r'sk_live_[a-zA-Z0-9]{24,}', re.IGNORECASE),
    "google_api": re.compile(r'AIza[a-zA-Z0-9_\-]{35}', re.IGNORECASE),
}


# =========================================
# CLASSIFICAÃ‡ÃƒO AVANÃ‡ADA DE ARQUIVOS
# =========================================
def classify_file(key: str, size: int, etag: str = "") -> dict:
    """
    Classifica o arquivo com anÃ¡lise aprimorada:
    - categoria (12 categorias distintas)
    - severidade (CrÃ­tica, Alta, MÃ©dia, Baixa)
    - cvss score (0.0 - 10.0)
    - recomendaÃ§Ãµes especÃ­ficas
    - detecÃ§Ã£o de padrÃµes sensÃ­veis no nome
    """
    name = key.split("/")[-1]
    path_lower = key.lower()
    name_lower = name.lower()
    
    if "." in name:
        ext = name.rsplit(".", 1)[-1].lower()
    else:
        ext = ""

    category = "Outros"
    risk = "Baixa"
    cvss = 2.0
    recommendations = []
    tags = []

    # ExtensÃµes categorizadas
    img_ext = {"jpg", "jpeg", "png", "gif", "svg", "ico", "bmp", "webp", "avif", "tiff", "heic"}
    cfg_ext = {"cfg", "conf", "config", "ini", "yaml", "yml", "json", "xml", "properties", "toml", "env"}
    key_ext = {"pem", "key", "ppk", "pfx", "p12", "crt", "cer", "der", "pub", "asc", "gpg"}
    backup_ext = {"sql", "bak", "dump", "gz", "zip", "7z", "rar", "tar", "tgz", "bz2", "backup"}
    font_ext = {"woff", "woff2", "ttf", "eot", "otf"}
    static_ext = {"css", "js", "map", "html", "htm"}
    doc_ext = {"pdf", "doc", "docx", "xls", "xlsx", "csv", "txt", "md", "odt", "ods", "ppt", "pptx"}
    code_ext = {"py", "java", "cpp", "c", "h", "rb", "php", "go", "rs", "ts", "jsx", "tsx", "vue"}
    video_ext = {"mp4", "avi", "mov", "wmv", "flv", "mkv", "webm"}
    audio_ext = {"mp3", "wav", "flac", "aac", "ogg", "m4a"}
    compressed_ext = {"zip", "rar", "7z", "tar", "gz", "bz2", "xz"}
    
    # Arquivos crÃ­ticos especÃ­ficos
    critical_files = {
        ".env", ".env.local", ".env.production", ".env.development", ".env.staging",
        "credentials", ".aws/credentials", "id_rsa", "id_ed25519", "id_ecdsa",
        ".ssh/id_rsa", "secrets.json", "secret.json", "private.key", "privatekey.pem",
        "apikeys.json", "service-account.json", "firebase-adminsdk.json",
        ".npmrc", ".pypirc", "shadow", "passwd", "htpasswd"
    }
    
    # Palavras-chave suspeitas
    suspicious_keywords = [
        "secret", "token", "password", "passwd", "pwd", "apikey", "api_key",
        "private", "credential", "auth", "bearer", "oauth"
    ]

    # ANÃLISE DE NOME DO ARQUIVO
    contains_suspicious = any(word in name_lower for word in suspicious_keywords)
    
    # ======================================
    # VERIFICAÃ‡ÃƒO 1: Arquivos crÃ­ticos especÃ­ficos (PRIORIDADE MÃXIMA)
    # ======================================
    if any(cf in path_lower for cf in critical_files):
        category = "ðŸ”´ Chaves/Credenciais"
        risk = "CrÃ­tica"
        cvss = 10.0
        tags.append("EXPOSIÃ‡ÃƒO_CRÃTICA")
        recommendations.extend([
            "ðŸš¨ URGENTE: Remova este arquivo IMEDIATAMENTE do bucket",
            "ðŸ”„ Rotacione TODAS as credenciais que possam estar neste arquivo",
            "ðŸ“Š Audite logs de acesso (CloudTrail) para identificar acessos nÃ£o autorizados",
            "ðŸ”’ Se o arquivo for necessÃ¡rio, mova para AWS Secrets Manager",
            "ðŸ“§ Considere notificar equipe de seguranÃ§a sobre exposiÃ§Ã£o"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 2: ExtensÃµes de chaves/certificados
    # ======================================
    elif ext in key_ext:
        category = "ðŸ”´ Chaves/Credenciais"
        risk = "CrÃ­tica"
        cvss = 9.8
        tags.append("CHAVE_PRIVADA")
        recommendations.extend([
            "ðŸ” Remova chaves privadas/certificados do bucket pÃºblico",
            "ðŸ”„ Rotacione as chaves comprometidas imediatamente",
            "â˜ï¸ Use AWS Secrets Manager ou Systems Manager Parameter Store",
            "ðŸ“ Documente o incidente de exposiÃ§Ã£o"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 3: Nome suspeito de credenciais
    # ======================================
    elif contains_suspicious:
        category = "ðŸ”´ Chaves/Credenciais"
        risk = "CrÃ­tica"
        cvss = 9.5
        tags.append("NOME_SUSPEITO")
        recommendations.extend([
            "âš ï¸ Arquivo com nome indicativo de conter credenciais",
            "ðŸ” FaÃ§a anÃ¡lise do conteÃºdo para confirmar exposiÃ§Ã£o",
            "ðŸ”„ Se confirmado, rotacione credenciais",
            "ðŸ“‹ Implemente varredura de secrets no CI/CD"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 4: DiretÃ³rio .git exposto
    # ======================================
    elif ".git/" in path_lower or path_lower.endswith(".git"):
        category = "ðŸ”´ RepositÃ³rio"
        risk = "CrÃ­tica"
        cvss = 9.0
        tags.append("GIT_EXPOSTO")
        recommendations.extend([
            "ðŸ’¥ RepositÃ³rio .git exposto permite reconstruÃ§Ã£o completa do cÃ³digo",
            "ðŸ—‘ï¸ Remova TODOS os arquivos .git/ do bucket",
            "ðŸ” Verifique se hÃ¡ credenciais no histÃ³rico de commits",
            "ðŸ›¡ï¸ Configure .gitignore adequadamente"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 5: Arquivos de configuraÃ§Ã£o
    # ======================================
    elif ext in cfg_ext:
        category = "âš ï¸ ConfiguraÃ§Ãµes"
        risk = "Alta"
        cvss = 8.0
        tags.append("CONFIG")
        
        # Aumenta severidade para .env e similares
        if ext == "env" or ".env" in name_lower:
            risk = "CrÃ­tica"
            cvss = 9.5
            category = "ðŸ”´ Chaves/Credenciais"
            tags.append("ENV_FILE")
        
        recommendations.extend([
            "ðŸ” Revise o conteÃºdo: pode conter credenciais hardcoded",
            "ðŸ” Use variÃ¡veis de ambiente e serviÃ§os de secrets",
            "ðŸ“ Documente configuraÃ§Ãµes sensÃ­veis que nÃ£o devem ser expostas",
            "ðŸ”’ Se necessÃ¡rio expor configs, remova dados sensÃ­veis"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 6: Backups de banco de dados
    # ======================================
    elif ext in backup_ext or any(x in name_lower for x in ["backup", "bkp", "dump"]):
        category = "âš ï¸ Backups"
        risk = "Alta"
        cvss = 8.5
        tags.append("BACKUP")
        
        # SQL dumps sÃ£o mais crÃ­ticos
        if ext == "sql" or "dump" in name_lower:
            cvss = 9.0
            tags.append("SQL_DUMP")
            recommendations.append("ðŸ’¾ SQL dumps podem conter dados sensÃ­veis completos")
        
        recommendations.extend([
            "ðŸ” Backups DEVEM ser criptografados (SSE-KMS)",
            "ðŸ”’ Mova para bucket privado com acesso restrito",
            "ðŸ“… Implemente polÃ­ticas de retenÃ§Ã£o e lifecycle",
            "ðŸ—‘ï¸ Configure expurgo automÃ¡tico de backups antigos"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 7: CÃ³digo-fonte
    # ======================================
    elif ext in code_ext:
        category = "âš ï¸ CÃ³digo-fonte"
        risk = "Alta"
        cvss = 7.5
        tags.append("SOURCE_CODE")
        recommendations.extend([
            "ðŸ’» CÃ³digo-fonte exposto revela lÃ³gica de negÃ³cio e vulnerabilidades",
            "ðŸ” Pode conter comentÃ¡rios com informaÃ§Ãµes sensÃ­veis",
            "ðŸš« Nunca exponha cÃ³digo-fonte em buckets pÃºblicos",
            "ðŸ“¦ Use repositÃ³rios privados (GitHub/GitLab/CodeCommit)"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 8: Arquivos comprimidos grandes
    # ======================================
    elif ext in compressed_ext:
        category = "ðŸ“¦ Comprimidos"
        risk = "MÃ©dia"
        cvss = 6.0
        tags.append("COMPRESSED")
        
        if size > 100 * 1024 * 1024:  # > 100MB
            cvss = 7.0
            risk = "Alta"
            recommendations.append("ðŸ“¦ Arquivo comprimido grande pode conter mÃºltiplos arquivos sensÃ­veis")
        
        recommendations.extend([
            "ðŸ” Audite o conteÃºdo do arquivo comprimido",
            "ðŸ” Se contÃ©m dados sensÃ­veis, criptografe e restrinja acesso"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 9: Documentos
    # ======================================
    elif ext in doc_ext:
        category = "ðŸ“„ Documentos"
        risk = "MÃ©dia"
        cvss = 5.5
        tags.append("DOCUMENT")
        
        # PDFs e planilhas podem conter dados sensÃ­veis
        if ext in ["pdf", "xlsx", "xls", "csv"]:
            cvss = 6.5
            recommendations.append("ðŸ“Š Documentos podem conter informaÃ§Ãµes confidenciais ou PII")
        
        recommendations.extend([
            "ðŸ” Revise se nÃ£o contÃ©m dados pessoais (LGPD/GDPR)",
            "ðŸ”’ Considere aplicar watermarks ou DRM se necessÃ¡rio"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 10: Source maps
    # ======================================
    elif ext == "map" or name.endswith(".map"):
        category = "âš ï¸ Source Maps"
        risk = "MÃ©dia"
        cvss = 6.0
        tags.append("SOURCE_MAP")
        recommendations.extend([
            "ðŸ—ºï¸ Source maps expÃµem cÃ³digo-fonte original (prÃ©-minificaÃ§Ã£o)",
            "ðŸš« Remova source maps de produÃ§Ã£o",
            "âš™ï¸ Configure build para nÃ£o gerar .map em produÃ§Ã£o"
        ])

    # ======================================
    # VERIFICAÃ‡ÃƒO 11: VÃ­deos e Ã¡udios
    # ======================================
    elif ext in video_ext or ext in audio_ext:
        category = "ðŸŽ¬ MÃ­dia"
        risk = "Baixa"
        cvss = 2.5
        tags.append("MEDIA")
        
        if size > 500 * 1024 * 1024:  # > 500MB
            recommendations.append("ðŸ’° Arquivos grandes impactam custos de transferÃªncia/armazenamento")

    # ======================================
    # VERIFICAÃ‡ÃƒO 12: Fontes web
    # ======================================
    elif ext in font_ext:
        category = "ðŸ”¤ Fontes"
        risk = "Baixa"
        cvss = 1.5
        tags.append("FONT")

    # ======================================
    # VERIFICAÃ‡ÃƒO 13: Arquivos estÃ¡ticos (CSS/JS/HTML)
    # ======================================
    elif ext in static_ext:
        category = "ðŸ“± EstÃ¡ticos"
        risk = "Baixa"
        cvss = 2.0
        tags.append("STATIC")
        
        # JS pode conter lÃ³gica sensÃ­vel
        if ext == "js":
            cvss = 3.5
            recommendations.append("âš ï¸ JS pode conter lÃ³gica de negÃ³cio ou endpoints de API")

    # ======================================
    # VERIFICAÃ‡ÃƒO 14: Imagens
    # ======================================
    elif ext in img_ext:
        category = "ðŸ–¼ï¸ Imagens"
        risk = "Baixa"
        cvss = 2.0
        tags.append("IMAGE")

    # ======================================
    # VERIFICAÃ‡ÃƒO 15: Outros/Desconhecidos
    # ======================================
    else:
        category = "â“ Outros"
        risk = "MÃ©dia"
        cvss = 5.0
        tags.append("UNKNOWN")
        
        if size > 100 * 1024 * 1024:  # > 100MB
            recommendations.append("ðŸ’° Arquivo grande com extensÃ£o desconhecida")
        
        recommendations.append("ðŸ” Verifique manualmente a natureza deste arquivo")

    # ======================================
    # ANÃLISE DE TAMANHO (todos os arquivos)
    # ======================================
    if size > 1024 * 1024 * 1024:  # > 1GB
        recommendations.append(f"ðŸ’° Arquivo muito grande ({format_size(size)}) - revise necessidade")

    # ======================================
    # VERIFICAÃ‡ÃƒO DE ACESSO PÃšBLICO
    # ======================================
    if risk in ["CrÃ­tica", "Alta"]:
        recommendations.insert(0, "ðŸŒ Arquivo de risco elevado acessÃ­vel publicamente")

    return {
        "filename": key,
        "extension": ext,
        "size": size,
        "category": category,
        "risk": risk,
        "cvss": cvss,
        "recommendations": recommendations,
        "tags": tags,
        "etag": etag,
        "last_modified": None  # SerÃ¡ preenchido se disponÃ­vel
    }


# =========================================
# FUNÃ‡Ã•ES AUXILIARES
# =========================================
def format_size(bytes_value: int) -> str:
    """Formata bytes em formato legÃ­vel"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_value < 1024.0:
            return f"{bytes_value:.2f} {unit}"
        bytes_value /= 1024.0
    return f"{bytes_value:.2f} PB"


def calculate_risk_score(files: List[Dict]) -> float:
    """Calcula score de risco ponderado (0-10)"""
    if not files:
        return 0.0
    
    weights = {
        "CrÃ­tica": 10.0,
        "Alta": 7.5,
        "MÃ©dia": 5.0,
        "Baixa": 2.0
    }
    
    total_weight = sum(weights.get(f["risk"], 5.0) for f in files)
    return round(total_weight / len(files), 1)


# =========================================
# AUDITOR S3 APRIMORADO
# =========================================
class S3Auditor:
    def __init__(self, bucket: str):
        self.bucket = bucket.strip()
        self.region = "unknown"
        self.public_access = False
        self.files = []
        self.critical_findings = []
        self.scan_metadata = {
            "start_time": datetime.datetime.now(),
            "end_time": None,
            "duration_seconds": 0
        }

    def log(self, msg: str, level: str = "INFO"):
        """Log com emojis e nÃ­veis"""
        emoji_map = {
            "INFO": "â„¹ï¸",
            "WARNING": "âš ï¸",
            "ERROR": "âŒ",
            "CRITICAL": "ðŸš¨",
            "SUCCESS": "âœ…"
        }
        print(f"{emoji_map.get(level, 'â„¹ï¸')} {msg}")

    def _fetch(self, url: str, method: str = "GET", params=None) -> Optional[requests.Response]:
        """RequisiÃ§Ã£o HTTP com tratamento robusto de erros"""
        try:
            headers = {
                'User-Agent': 'S3-Security-Auditor/3.0',
                'Accept': '*/*'
            }
            
            if method == "GET":
                return requests.get(url, params=params, headers=headers, timeout=20)
            else:
                return requests.head(url, headers=headers, timeout=15)
                
        except requests.exceptions.Timeout:
            self.log(f"Timeout ao acessar {url[:50]}...", "WARNING")
        except requests.exceptions.ConnectionError:
            self.log(f"Erro de conexÃ£o", "ERROR")
        except requests.exceptions.RequestException as e:
            self.log(f"Erro na requisiÃ§Ã£o: {str(e)[:100]}", "ERROR")
        except Exception as e:
            self.log(f"Erro inesperado: {str(e)[:100]}", "ERROR")
            
        return None

    def validate_bucket_name(self) -> bool:
        """Valida nome do bucket segundo regras AWS S3"""
        # Verifica comprimento (3-63 caracteres)
        if not (3 <= len(self.bucket) <= 63):
            self.log(f"Nome do bucket deve ter entre 3-63 caracteres: {self.bucket}", "ERROR")
            return False
        
        # PadrÃ£o vÃ¡lido: letras minÃºsculas, nÃºmeros, hÃ­fens e pontos
        pattern = r'^[a-z0-9][a-z0-9\.\-]*[a-z0-9]$'
        if not re.match(pattern, self.bucket):
            self.log(f"Nome de bucket com caracteres invÃ¡lidos: {self.bucket}", "ERROR")
            return False
        
        # NÃ£o pode ter padrÃµes invÃ¡lidos
        if '..' in self.bucket or '.-' in self.bucket or '-.' in self.bucket:
            self.log(f"Nome contÃ©m padrÃµes invÃ¡lidos (.., .-, -.): {self.bucket}", "ERROR")
            return False
        
        # NÃ£o pode parecer um IP
        if re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', self.bucket):
            self.log(f"Nome nÃ£o pode ser formato de IP: {self.bucket}", "ERROR")
            return False
            
        return True

    def detect_region(self):
        """Detecta regiÃ£o do bucket com mÃºltiplas tentativas"""
        self.log("Detectando regiÃ£o...")
        
        # Tentativa 1: HEAD request no endpoint genÃ©rico
        url = f"https://{self.bucket}.s3.amazonaws.com"
        resp = self._fetch(url, "HEAD")
        
        if resp and "x-amz-bucket-region" in resp.headers:
            self.region = resp.headers["x-amz-bucket-region"]
            self.log(f"RegiÃ£o detectada: {self.region}", "SUCCESS")
            return
        
        # Tentativa 2: GET request simples
        resp = self._fetch(url, "GET", params={"max-keys": "0"})
        if resp and "x-amz-bucket-region" in resp.headers:
            self.region = resp.headers["x-amz-bucket-region"]
            self.log(f"RegiÃ£o detectada: {self.region}", "SUCCESS")
            return
        
        # Se nÃ£o conseguir detectar, usa endpoint genÃ©rico
        self.log("NÃ£o foi possÃ­vel detectar a regiÃ£o (usando endpoint genÃ©rico)", "WARNING")
        self.region = "us-east-1"  # Fallback

    def check_public_access(self):
        """Verifica mÃºltiplos aspectos de acesso pÃºblico"""
        self.log("Verificando acesso pÃºblico...")
        
        # Monta URL baseado na regiÃ£o
        if self.region and self.region != "unknown":
            base = f"https://{self.bucket}.s3.{self.region}.amazonaws.com"
        else:
            base = f"https://{self.bucket}.s3.amazonaws.com"

        # Tenta listar objetos
        params = {"list-type": "2", "max-keys": "1"}
        resp = self._fetch(base, "GET", params=params)

        if resp and resp.status_code == 200:
            if "<ListBucketResult" in resp.text and "AccessDenied" not in resp.text:
                self.public_access = True
                self.log("ðŸš¨ ATENÃ‡ÃƒO: Acesso pÃºblico PERMITIDO!", "CRITICAL")
                
                self.critical_findings.append({
                    "type": "public_bucket",
                    "severity": "CRITICAL",
                    "cvss": 9.0,
                    "message": "Bucket permite listagem pÃºblica de objetos (List)",
                    "remediation": "Ative Block Public Access nas configuraÃ§Ãµes do bucket"
                })
            else:
                self.public_access = False
                self.log("ðŸ”’ Acesso pÃºblico: BLOQUEADO", "SUCCESS")
        else:
            self.public_access = False
            status = resp.status_code if resp else "N/A"
            self.log(f"NÃ£o foi possÃ­vel verificar acesso pÃºblico (HTTP {status})", "WARNING")

    def deep_scan_http(self, max_files: Optional[int] = None, check_content: bool = False):
        """
        Varredura profunda com opÃ§Ãµes avanÃ§adas
        
        Args:
            max_files: Limite de arquivos a processar (None = sem limite)
            check_content: Se True, tenta baixar e analisar conteÃºdo de arquivos suspeitos
        """
        if not self.public_access:
            self.log("Deep scan cancelado: bucket nÃ£o permite listagem pÃºblica", "WARNING")
            return

        self.log("ðŸ” Executando deep scan HTTP...")

        # Determina URL base
        if self.region and self.region != "unknown":
            base = f"https://{self.bucket}.s3.{self.region}.amazonaws.com"
        else:
            base = f"https://{self.bucket}.s3.amazonaws.com"

        params = {"list-type": "2", "max-keys": "1000"}
        continuation_token = None
        total = 0
        critical_count = 0
        high_count = 0

        while True:
            # Atualiza token de continuaÃ§Ã£o
            if continuation_token:
                params["continuation-token"] = continuation_token
            elif "continuation-token" in params:
                del params["continuation-token"]

            # Faz requisiÃ§Ã£o
            resp = self._fetch(base, "GET", params=params)
            if not resp or resp.status_code != 200:
                self.log(f"Falha ao listar objetos (HTTP {resp.status_code if resp else 'N/A'})", "ERROR")
                break

            # Parse XML
            try:
                xml_root = ET.fromstring(resp.text)
            except ET.ParseError as e:
                self.log(f"Erro ao parsear XML: {e}", "ERROR")
                break

            # Processa objetos
            contents = xml_root.findall(".//{http://s3.amazonaws.com/doc/2006-03-01/}Contents")
            if not contents:
                # Tenta sem namespace
                contents = xml_root.findall(".//Contents")

            for c in contents:
                # Extrai metadados
                key_el = c.find("{http://s3.amazonaws.com/doc/2006-03-01/}Key")
                if key_el is None:
                    key_el = c.find("Key")
                
                size_el = c.find("{http://s3.amazonaws.com/doc/2006-03-01/}Size")
                if size_el is None:
                    size_el = c.find("Size")
                
                etag_el = c.find("{http://s3.amazonaws.com/doc/2006-03-01/}ETag")
                if etag_el is None:
                    etag_el = c.find("ETag")
                
                modified_el = c.find("{http://s3.amazonaws.com/doc/2006-03-01/}LastModified")
                if modified_el is None:
                    modified_el = c.find("LastModified")
                
                if key_el is None:
                    continue
                    
                key = key_el.text
                try:
                    size = int(size_el.text) if size_el is not None else 0
                except (ValueError, TypeError):
                    size = 0
                
                etag = etag_el.text.strip('"') if etag_el is not None else ""
                last_modified = modified_el.text if modified_el is not None else None

                # Classifica arquivo
                file_info = classify_file(key, size, etag)
                file_info["last_modified"] = last_modified
                
                # Adiciona URL de acesso
                file_info["url"] = f"{base}/{key}"
                
                self.files.append(file_info)
                total += 1

                # Contadores por severidade
                if file_info["risk"] == "CrÃ­tica":
                    critical_count += 1
                    self.critical_findings.append({
                        "type": "critical_file",
                        "severity": "CRITICAL",
                        "cvss": file_info["cvss"],
                        "file": key,
                        "category": file_info["category"],
                        "size": size,
                        "recommendations": file_info["recommendations"]
                    })
                    self.log(f"ðŸš¨ CRÃTICO: {key}", "CRITICAL")
                    
                elif file_info["risk"] == "Alta":
                    high_count += 1

                # Limite de arquivos
                if max_files and total >= max_files:
                    break

            # Log de progresso
            if total % 1000 == 0:
                self.log(f"ðŸ“Š Processados: {total:,} arquivos ({critical_count} crÃ­ticos, {high_count} altos)")

            # Verifica limite
            if max_files and total >= max_files:
                self.log(f"âš ï¸ Limite de {max_files:,} arquivos atingido", "WARNING")
                break

            # Verifica paginaÃ§Ã£o
            is_trunc = xml_root.find(".//{http://s3.amazonaws.com/doc/2006-03-01/}IsTruncated")
            if is_trunc is None:
                is_trunc = xml_root.find(".//IsTruncated")
                
            if is_trunc is not None and is_trunc.text == "true":
                token_el = xml_root.find(".//{http://s3.amazonaws.com/doc/2006-03-01/}NextContinuationToken")
                if token_el is None:
                    token_el = xml_root.find(".//NextContinuationToken")
                    
                if token_el is not None and token_el.text:
                    continuation_token = token_el.text
                    continue
            
            # Fim da paginaÃ§Ã£o
            break

        self.log(f"âœ… Scan finalizado: {total:,} arquivos ({critical_count} crÃ­ticos, {high_count} altos)", "SUCCESS")

    def compute_summary(self) -> dict:
        """Calcula estatÃ­sticas detalhadas do scan"""
        risk_counts = Counter(f["risk"] for f in self.files)
        category_counts = Counter(f["category"] for f in self.files)
        extension_counts = Counter(f["extension"] for f in self.files if f["extension"])
        total_files = len(self.files)

        # CVSS mÃ©dio por severidade
        cvss_by_risk = defaultdict(list)
        for f in self.files:
            cvss_by_risk[f["risk"]].append(f["cvss"])
        
        avg_cvss_by_risk = {
            risk: round(sum(values) / len(values), 1) if values else 0.0
            for risk, values in cvss_by_risk.items()
        }

        # Score de risco ponderado
        risk_score = calculate_risk_score(self.files)

        # Tamanho total e por categoria
        total_size = sum(f["size"] for f in self.files)
        size_by_category = defaultdict(int)
        for f in self.files:
            size_by_category[f["category"]] += f["size"]

        # Top 10 maiores arquivos
        largest_files = sorted(self.files, key=lambda x: x["size"], reverse=True)[:10]

        # Arquivos mais crÃ­ticos (top 20)
        most_critical = sorted(
            [f for f in self.files if f["risk"] in ["CrÃ­tica", "Alta"]],
            key=lambda x: x["cvss"],
            reverse=True
        )[:20]

        return {
            "total_files": total_files,
            "total_size": total_size,
            "total_size_formatted": format_size(total_size),
            "risk_counts": dict(risk_counts),
            "category_counts": dict(category_counts),
            "extension_counts": dict(extension_counts.most_common(20)),
            "avg_cvss_by_risk": avg_cvss_by_risk,
            "risk_score": risk_score,
            "critical_findings_count": len(self.critical_findings),
            "size_by_category": dict(size_by_category),
            "largest_files": [
                {"filename": f["filename"], "size": f["size"], "size_formatted": format_size(f["size"])}
                for f in largest_files
            ],
            "most_critical": [
                {"filename": f["filename"], "cvss": f["cvss"], "risk": f["risk"], "category": f["category"]}
                for f in most_critical
            ]
        }

    def update_history(self, summary: dict) -> list:
        """MantÃ©m histÃ³rico das Ãºltimas 100 execuÃ§Ãµes com mais metadados"""
        history_file = os.path.join(HISTORY_FOLDER, f"{self.bucket}.json")
        
        try:
            if os.path.exists(history_file):
                with open(history_file, "r", encoding="utf-8") as f:
                    history = json.load(f)
            else:
                history = []
        except (json.JSONDecodeError, IOError):
            self.log("HistÃ³rico corrompido ou inacessÃ­vel, criando novo", "WARNING")
            history = []

        entry = {
            "date": datetime.datetime.now().isoformat(timespec="seconds"),
            "total_files": summary["total_files"],
            "total_size": summary["total_size"],
            "risk_score": summary["risk_score"],
            "critical": summary["risk_counts"].get("CrÃ­tica", 0),
            "high": summary["risk_counts"].get("Alta", 0),
            "medium": summary["risk_counts"].get("MÃ©dia", 0),
            "low": summary["risk_counts"].get("Baixa", 0),
            "duration_seconds": self.scan_metadata.get("duration_seconds", 0)
        }
        history.append(entry)
        
        # MantÃ©m Ãºltimas 100 execuÃ§Ãµes
        history = history[-100:]

        try:
            with open(history_file, "w", encoding="utf-8") as f:
                json.dump(history, f, indent=2, ensure_ascii=False)
        except IOError as e:
            self.log(f"Erro ao salvar histÃ³rico: {e}", "ERROR")

        return history

    def generate_recommendations(self, summary: dict) -> List[str]:
        """Gera recomendaÃ§Ãµes personalizadas baseadas nos achados"""
        recommendations = []
        
        critical = summary["risk_counts"].get("CrÃ­tica", 0)
        high = summary["risk_counts"].get("Alta", 0)
        
        # RecomendaÃ§Ãµes baseadas em severidade
        if critical > 0:
            recommendations.append(f"ðŸš¨ **URGENTE**: {critical} arquivo(s) crÃ­tico(s) detectado(s) â€” AÃ§Ã£o imediata necessÃ¡ria!")
            recommendations.append("ðŸ”„ Rotacione todas as credenciais potencialmente expostas")
            recommendations.append("ðŸ“Š Audite CloudTrail logs para verificar acessos nÃ£o autorizados")
        
        if high > 0:
            recommendations.append(f"âš ï¸ **ATENÃ‡ÃƒO**: {high} arquivo(s) de risco alto â€” Priorize revisÃ£o")
        
        # RecomendaÃ§Ãµes gerais de seguranÃ§a
        if self.public_access:
            recommendations.append("ðŸ”’ **Ative Block Public Access** no bucket (4 opÃ§Ãµes)")
        
        recommendations.extend([
            "ðŸ“ Habilite **Server Access Logging** e **AWS CloudTrail**",
            "ðŸ” Implemente **polÃ­ticas IAM de menor privilÃ©gio**",
            "ðŸ›¡ï¸ Use **AWS Secrets Manager** para credenciais",
            "ðŸ”„ Habilite **versionamento** do bucket",
            "ðŸ”‘ Configure **criptografia SSE-KMS** com chaves gerenciadas",
            "ðŸ¤– Configure **Amazon Macie** para descoberta de dados sensÃ­veis",
            "âš™ï¸ Implemente **polÃ­ticas de lifecycle** para expurgo automÃ¡tico",
            "ðŸ” Configure **AWS Config Rules** para conformidade contÃ­nua",
            "ðŸš¨ Habilite **AWS GuardDuty** para detecÃ§Ã£o de ameaÃ§as",
            "ðŸ“‹ Documente todos os arquivos legÃ­timos que devem permanecer"
        ])
        
        # RecomendaÃ§Ãµes especÃ­ficas por categoria
        if summary["category_counts"].get("ðŸ”´ Chaves/Credenciais", 0) > 0:
            recommendations.append("âš ï¸ Implante **git-secrets** e **truffleHog** no CI/CD")
        
        if summary["category_counts"].get("âš ï¸ Backups", 0) > 0:
            recommendations.append("ðŸ’¾ Mova backups para bucket dedicado com replicaÃ§Ã£o cross-region")
        
        if summary["total_size"] > 100 * 1024 * 1024 * 1024:  # > 100GB
            recommendations.append("ðŸ’° Considere **S3 Intelligent-Tiering** para otimizar custos")
        
        return recommendations

    def export_json_and_html(self):
        """Exporta relatÃ³rios JSON e HTML completos"""
        # Calcula tempo de execuÃ§Ã£o
        self.scan_metadata["end_time"] = datetime.datetime.now()
        self.scan_metadata["duration_seconds"] = (
            self.scan_metadata["end_time"] - self.scan_metadata["start_time"]
        ).total_seconds()
        
        summary = self.compute_summary()
        history = self.update_history(summary)
        recommendations = self.generate_recommendations(summary)

        # Estrutura do relatÃ³rio
        report = {
            "bucket": self.bucket,
            "region": self.region,
            "public_access": self.public_access,
            "generated_at": datetime.datetime.now().isoformat(timespec="seconds"),
            "scan_duration_seconds": round(self.scan_metadata["duration_seconds"], 2),
            "auditor_version": "3.0",
            "files": self.files,
            "summary": summary,
            "history": history,
            "critical_findings": self.critical_findings,
            "recommendations": recommendations
        }

        # Exporta JSON
        json_name = f"{self.bucket}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        json_path = os.path.join(REPORT_FOLDER, json_name)

        try:
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.log(f"ðŸ“„ JSON gerado: reports/{json_name}", "SUCCESS")
        except IOError as e:
            self.log(f"Erro ao salvar JSON: {e}", "ERROR")
            return

        # Gera HTML
        if not os.path.exists(TEMPLATE_FILE):
            self.log("âš ï¸ Template dashboard.html nÃ£o encontrado; HTML nÃ£o serÃ¡ gerado", "WARNING")
            return

        try:
            with open(TEMPLATE_FILE, "r", encoding="utf-8") as f:
                template = f.read()

            html = template.replace("__BUCKET_NAME__", self.bucket).replace("__REPORT_JSON__", json_name)

            html_name = f"{self.bucket}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            html_path = os.path.join(REPORT_FOLDER, html_name)

            with open(html_path, "w", encoding="utf-8") as f:
                f.write(html)

            self.log(f"ðŸŒ HTML exportado: reports/{html_name}", "SUCCESS")
        except IOError as e:
            self.log(f"Erro ao gerar HTML: {e}", "ERROR")

    def print_summary(self):
        """Exibe resumo executivo detalhado no console"""
        summary = self.compute_summary()
        
        print("\n" + "="*70)
        print(f"ðŸ“Š RESUMO EXECUTIVO DA AUDITORIA")
        print("="*70)
        print(f"ðŸª£ Bucket: {self.bucket}")
        print(f"ðŸŒ RegiÃ£o: {self.region}")
        print(f"ðŸ”“ Acesso pÃºblico: {'SIM âš ï¸' if self.public_access else 'NÃƒO âœ…'}")
        print(f"â±ï¸  DuraÃ§Ã£o do scan: {self.scan_metadata.get('duration_seconds', 0):.1f}s")
        print("="*70)
        
        print(f"\nðŸ“ ARQUIVOS:")
        print(f"  Total: {summary['total_files']:,}")
        print(f"  Tamanho total: {summary['total_size_formatted']}")
        
        print(f"\nâš ï¸  DISTRIBUIÃ‡ÃƒO DE RISCO:")
        for risk in ["CrÃ­tica", "Alta", "MÃ©dia", "Baixa"]:
            count = summary['risk_counts'].get(risk, 0)
            if count > 0:
                pct = (count / summary['total_files'] * 100) if summary['total_files'] > 0 else 0
                emoji = {"CrÃ­tica": "ðŸš¨", "Alta": "âš ï¸", "MÃ©dia": "â„¹ï¸", "Baixa": "âœ…"}
                print(f"  {emoji.get(risk, 'â€¢')} {risk}: {count:,} ({pct:.1f}%)")
        
        print(f"\nðŸŽ¯ SCORE DE RISCO: {summary['risk_score']}/10")
        
        if self.critical_findings:
            print(f"\nðŸš¨ DESCOBERTAS CRÃTICAS: {len(self.critical_findings)}")
            for finding in self.critical_findings[:5]:  # Mostra as 5 primeiras
                print(f"  â€¢ {finding.get('message', finding.get('file', 'Unknown'))}")
            if len(self.critical_findings) > 5:
                print(f"  ... e mais {len(self.critical_findings) - 5} descoberta(s)")
        
        print("\n" + "="*70)
        print("âœ… Auditoria concluÃ­da! RelatÃ³rios exportados.")
        print("="*70 + "\n")

    def run(self, max_files: Optional[int] = None):
        """Executa auditoria completa"""
        print(f"\n{'='*70}")
        print(f"ðŸ” AUDITORIA DE SEGURANÃ‡A S3 v3.0")
        print(f"{'='*70}")
        print(f"ðŸª£ Bucket: {self.bucket}")
        print(f"ðŸ“… Data: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}\n")
        
        # ValidaÃ§Ãµes
        if not self.validate_bucket_name():
            self.log("âŒ Auditoria abortada: nome de bucket invÃ¡lido", "ERROR")
            return
        
        # Executa etapas
        self.detect_region()
        self.check_public_access()
        self.deep_scan_http(max_files=max_files)
        
        # Exporta resultados
        if self.files:
            self.export_json_and_html()
            self.print_summary()
        else:
            self.log("âš ï¸ Nenhum arquivo encontrado ou bucket inacessÃ­vel", "WARNING")
        
        print(f"âœ… Auditoria concluÃ­da em {self.scan_metadata.get('duration_seconds', 0):.1f}s\n")


# =========================================
# MAIN
# =========================================
if __name__ == "__main__":
    print("\nðŸ” S3 Security Auditor v3.0 - Enterprise Edition")
    print("="*70)
    print("ðŸ›¡ï¸  Auditoria avanÃ§ada de seguranÃ§a para buckets AWS S3")
    print("="*70 + "\n")
    
    entry = input("ðŸª£ Digite 1 ou mais buckets (separados por vÃ­rgula): ").strip()
    
    if not entry:
        print("âŒ Nenhum bucket informado.")
        exit(1)
    
    buckets = [b.strip() for b in entry.split(",") if b.strip()]
    
    # Pergunta sobre limite
    limit_input = input(f"\nðŸ”¢ Limite de arquivos por bucket? (Enter = sem limite): ").strip()
    max_files = None
    if limit_input.isdigit():
        max_files = int(limit_input)
    
    print(f"\nðŸ“‹ {len(buckets)} bucket(s) para auditar")
    if max_files:
        print(f"ðŸ”¢ Limite: {max_files:,} arquivos por bucket")
    print()
    
    for i, b in enumerate(buckets, 1):
        if len(buckets) > 1:
            print(f"\n{'='*70}")
            print(f"ðŸ“Š Auditando bucket {i}/{len(buckets)}")
            print(f"{'='*70}")
        
        auditor = S3Auditor(b)
        auditor.run(max_files=max_files)
        
        if i < len(buckets):
            print("\n" + "-"*70 + "\n")
    
    print(f"\nðŸŽ‰ Todas as auditorias concluÃ­das!")
    print(f"ðŸ“ RelatÃ³rios salvos em: {REPORT_FOLDER}\n")
